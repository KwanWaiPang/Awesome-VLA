<p align="center">
  <h1 align="center">
  Awesome Visual-Language-Action (VLA)
  </h1>
</p>

This repository contains a curated list of resources addressing the VLA (Visual Language Action).

If you find some ignored papers, **feel free to [*create pull requests*](https://github.com/KwanWaiPang/Awesome-Transformer-based-SLAM/blob/pdf/How-to-PR.md), or [*open issues*](https://github.com/KwanWaiPang/Awesome-VLN/issues/new)**. 

Contributions in any form to make this list more comprehensive are welcome.

If you find this repository useful, a simple star should be the best affirmation. ðŸ˜Š

Feel free to share this list with others!

# Overview
- [Efficient-VLA](#Efficient-VLA)
- [Survey Paper](#Survey-Paper)
- [Other Resources](#Other-Resources)

<br>

<!-- |---|`arXiv`|---|---|---| -->
<!-- [![Github stars](https://img.shields.io/github/stars/***.svg)]() -->
| Year | Venue | Paper Title | Repository | Note |
|:----:|:-----:| ----------- |:----------:|:----:|
|2025|`CoRL`|[Ï€0.5: a Vision-Language-Action Model with Open-World Generalization](https://openreview.net/pdf?id=vlhoswksBO)|[![Github stars](https://img.shields.io/github/stars/Physical-Intelligence/openpi.svg)](https://github.com/Physical-Intelligence/openpi)|[Blog](https://www.pi.website/blog/pi05)<br>PI0.5|
|2025|`arXiv`|[Pure Vision Language Action (VLA) Models: A Comprehensive Survey](https://arxiv.org/pdf/2509.19012)|---|---|
|2025|`arXiv`|[InternVLA-M1: Latent Spatial Grounding for Instruction-Following Robotic Manipulation](https://arxiv.org/pdf/2510.13778)|[![Github stars](https://img.shields.io/github/stars/InternRobotics/InternVLA-M1.svg)](https://github.com/InternRobotics/InternVLA-M1)|[Website](https://internrobotics.github.io/internvla-m1.github.io/)|
|2025|`arXiv`|[AnywhereVLA: Language-Conditioned Exploration and Mobile Manipulation](https://arxiv.org/pdf/2509.21006)|[![Github stars](https://img.shields.io/github/stars/SelfAI-research/AnywhereVLA.svg)](https://github.com/SelfAI-research/AnywhereVLA)|[website](https://selfai-research.github.io/AnywhereVLA/)|
|2025|`arXiv`|[Hi robot: Open-ended instruction following with hierarchical vision-language-action models](https://arxiv.org/pdf/2502.19417)|---|[Website](https://www.pi.website/research/hirobot)|
|2025|`arXiv`|[Fast: Efficient action tokenization for vision-language-action models](https://arxiv.org/pdf/2501.09747)|[![Github stars](https://img.shields.io/github/stars/Physical-Intelligence/openpi.svg)](https://github.com/Physical-Intelligence/openpi)|[Website](https://www.pi.website/research/fast)<br>PI0-Fast|
|2025|`arXiv`|[Agibot world colosseo: A large-scale manipulation platform for scalable and intelligent embodied systems](https://arxiv.org/pdf/2503.06669)|[![Github stars](https://img.shields.io/github/stars/OpenDriveLab/AgiBot-World.svg)](https://github.com/OpenDriveLab/AgiBot-World)|---|
|2025|`CoRL`|[OpenVLA: An Open-Source Vision-Language-Action Model](https://arxiv.org/pdf/2406.09246?)|[![Github stars](https://img.shields.io/github/stars/openvla/openvla.svg)](https://github.com/openvla/openvla)|[Website](https://openvla.github.io/)| 
|2024|`RSS`|[Octo: An Open-Source Generalist Robot Policy](https://arxiv.org/pdf/2405.12213)|[![Github stars](https://img.shields.io/github/stars/octo-models/octo.svg)](https://github.com/octo-models/octo)|[Website](https://octo-models.github.io/)|
|2024|`CoRR`|[Ï€0: A Vision-Language-Action Flow Model for General Robot Control](https://arxiv.org/pdf/2410.24164)|[![Github stars](https://img.shields.io/github/stars/Physical-Intelligence/openpi.svg)](https://github.com/Physical-Intelligence/openpi)|[Blog](https://physicalintelligence.company/blog/pi0)<br>PI0|
|2023|`CoRL`|[Rt-2: Vision-language-action models transfer web knowledge to robotic control](https://robotics-transformer2.github.io/assets/rt2.pdf)|---|[Website](https://robotics-transformer2.github.io/)|
|2023|`RSS`|[Learning fine-grained bimanual manipulation with low-cost hardware](https://arxiv.org/pdf/2304.13705)|[![Github stars](https://img.shields.io/github/stars/tonyzhaozh/act.svg)](https://github.com/tonyzhaozh/act)|[Website](https://tonyzhaozh.github.io/aloha/)<br>ALOHA/ACT|
|2022|`arXiv`|[Rt-1: Robotics transformer for real-world control at scale](https://arxiv.org/pdf/2212.06817)|[![Github stars](https://img.shields.io/github/stars/google-research/robotics_transformer.svg)](https://github.com/google-research/robotics_transformer)|[website](https://robotics-transformer1.github.io/)<br>[Blog](https://research.google/blog/rt-1-robotics-transformer-for-real-world-control-at-scale/)|




# Efficient-VLA

<!-- |---|`arXiv`|---|---|---| -->
<!-- [![Github stars](https://img.shields.io/github/stars/***.svg)]() -->
| Year | Venue | Paper Title | Repository | Note |
|:----:|:-----:| ----------- |:----------:|:----:|
|2025|`arXiv`|[Running VLAs at Real-time Speed](https://arxiv.org/pdf/2510.26742)|[![Github stars](https://img.shields.io/github/stars/Dexmal/realtime-vla.svg)](https://github.com/Dexmal/realtime-vla)|[Blog](https://kwanwaipang.github.io/realtime-vla/)|
|2025|`arXiv`|[NanoVLA: Routing Decoupled Vision-Language Understanding for Nano-sized Generalist Robotic Policies](https://arxiv.org/pdf/2510.25122v1)|---|---|
|---|`arXiv`|---|---|---|
|2025|`arXiv`|[Unified Vision-Language-Action Model](https://arxiv.org/pdf/2506.19850)|[![Github stars](https://img.shields.io/github/stars/baaivision/UniVLA.svg)](https://github.com/baaivision/UniVLA) |[website](https://robertwyq.github.io/univla.github.io/)|
|2025|`ICML`|[Otter: A vision-language-action model with text-aware visual feature extraction](https://arxiv.org/pdf/2503.03734)|[![Github stars](https://img.shields.io/github/stars/Max-Fu/otter.svg)](https://github.com/Max-Fu/otter)|[Website](https://ottervla.github.io/)|
|2025|`arXiv`|[Sqap-vla: A synergistic quantization-aware pruning framework for high-performance vision-language-action models](https://arxiv.org/pdf/2509.09090?)|[![Github stars](https://img.shields.io/github/stars/ecdine/SQAP-VLA.svg)](https://github.com/ecdine/SQAP-VLA)|---|
|2025|`arXiv`|[Fastdrivevla: Efficient end-to-end driving via plug-and-play reconstruction-based token pruning](https://arxiv.org/pdf/2507.23318)|---|---|
| 2025 | `RAL` | [TinyVLA: Towards Fast, Data-Efficient Vision-Language-Action Models for Robotic Manipulation](https://arxiv.org/abs/2409.12514) | [![Github stars](https://img.shields.io/github/stars/liyaxuanliyaxuan/TinyVLA.svg)](https://github.com/liyaxuanliyaxuan/TinyVLA) | [Website](https://tiny-vla.github.io/) |
|2025|`arXiv`|[Smolvla: A vision-language-action model for affordable and efficient robotics](https://arxiv.org/pdf/2506.01844)|---|[website](https://huggingface.co/blog/smolvla)|
| 2025 | `arXiv` | [NORA: A Small Open-Sourced Generalist Vision-Language-Action Model for Embodied Tasks](https://arxiv.org/abs/2504.19854) | [![Github stars](https://img.shields.io/github/stars/declare-lab/nora.svg)](https://github.com/declare-lab/nora) | [Website](https://declare-lab.github.io/nora) |
| 2025 | `arXiv` | [MoLe-VLA: Dynamic Layer-Skipping Vision-Language-Action Model via Mixture-of-Layers for Efficient Robot Manipulation](https://arxiv.org/abs/2503.20384) | [![Github stars](https://img.shields.io/github/stars/RoyZry98/MoLe-VLA-Pytorch.svg)](https://github.com/RoyZry98/MoLe-VLA-Pytorch) | [Website](https://sites.google.com/view/mole-vla) |
| 2025 | `arXiv` | [EfficientVLA: Trainingâ€‘Free Acceleration and Compression for Visionâ€‘Languageâ€‘Action Models](https://arxiv.org/abs/2506.10100) | --- | --- |
| 2025 | `arXiv` | [Think Twice, Act Once: Tokenâ€‘Aware Compression and Action Reuse for Efficient Inference in Visionâ€‘Languageâ€‘Action Models](https://arxiv.org/abs/2505.21200) | --- | --- |
| 2025 | `arXiv` | [ThinkAct: Vision-Language-Action Reasoning via Reinforced Visual Latent Planning](https://arxiv.org/abs/2507.16815) | --- | [Website](https://jasper0314-huang.github.io/thinkact-vla/) |
| 2025 | `arXiv` | [OpenHelix: A Short Survey, Empirical Analysis, and Open-Source Dual-System VLA Model for Robotic Manipulation](https://arxiv.org/abs/2505.03912) | [![Github stars](https://img.shields.io/github/stars/OpenHelix-robot/OpenHelix.svg)](https://github.com/OpenHelix-robot/OpenHelix) | [Website](https://openhelix-robot.github.io/) |
| 2025 | `arXiv` | [SpatialVLA: Exploring Spatial Representations for Visual-Language-Action Model](https://arxiv.org/abs/2501.15830) | [![Github stars](https://img.shields.io/github/stars/SpatialVLA/SpatialVLA.svg)](https://github.com/SpatialVLA/SpatialVLA) | [Website](https://spatialvla.github.io/) |
| 2025 | `arXiv` | [Fast-in-Slow: A Dual-System Foundation Model Unifying Fast Manipulation within Slow Reasoning](https://arxiv.org/abs/2506.01953) | [![Github stars](https://img.shields.io/github/stars/CHEN-H01/Fast-in-Slow.svg)](https://github.com/CHEN-H01/Fast-in-Slow) | [Website](https://fast-in-slow.github.io/) |
| 2025 | `arXiv` | [SP-VLA: A Joint Model Scheduling and Token Pruning Approach for VLA Model Acceleration](https://arxiv.org/abs/2506.12723) | --- | --- |
| 2025 | `arXiv` | [VLAâ€‘Cache: Towards Efficient Visionâ€‘Languageâ€‘Action Model via Adaptive Token Caching in Robotic Manipulation](https://arxiv.org/abs/2502.02175) | [![Github stars](https://img.shields.io/github/stars/siyuhsu/vla-cache.svg)](https://github.com/siyuhsu/vla-cache) | --- |
| 2025 | `arXiv` | [SpecPrune-VLA: Accelerating Vision-Language-Action Models via Action-Aware Self-Speculative Pruning](https://arxiv.org/abs/2509.05614) | --- | --- |
| 2025 | `RSS` | [FAST: Efficient Action Tokenization for Vision-Language-Action Models](https://www.roboticsproceedings.org/rss21/p012.pdf) | --- | [Website](https://www.pi.website/research/fast) |
| 2025 | `arXiv` | [Real-Time Execution of Action Chunking Flow Policies](https://arxiv.org/abs/2506.07339) | --- | [Website](https://www.pi.website/research/real_time_chunking) |
| 2025 | `arXiv` | [VOTE: Vision-Language-Action Optimization with Trajectory Ensemble Voting](https://arxiv.org/abs/2507.05116) | --- | --- |
| 2025 | `arXiv` | [BitVLA: 1-Bit Vision-Language-Action Models for Robotics Manipulation](https://arxiv.org/abs/2506.07530) | [![Github stars](https://img.shields.io/github/stars/ustcwhy/BitVLA.svg)](https://github.com/ustcwhy/BitVLA) | --- |
| 2025 | `arXiv` | [SpecVLA: Speculative Decoding for Vision-Language-Action Models with Relaxed Acceptance](https://arxiv.org/abs/2507.22424) | --- | --- |
| 2024 | `NeurIPS` | [RoboMamba: Efficient Vision-Language-Action Model for Robotic Reasoning and Manipulation](https://arxiv.org/abs/2406.04339) | [![Github stars](https://img.shields.io/github/stars/lmzpai/roboMamba.svg)](https://github.com/lmzpai/roboMamba) | [Website](https://sites.google.com/view/robomamba-web) |
| 2024 | `NeurIPS` | [DeeR-VLA: Dynamic Inference of Multimodal Large Language Models for Efficient Robot Execution](https://arxiv.org/abs/2411.02359) | [![Github stars](https://img.shields.io/github/stars/yueyang130/DeeR-VLA.svg)](https://github.com/yueyang130/DeeR-VLA) | --- |
| 2024 | `IROS` | [From LLMs to Actions: Latent Codes as Bridges in Hierarchical Robot Control](https://arxiv.org/abs/2405.04798) | --- | --- |
| 2024 | `CoRL` | [HIRT: Enhancing Robotic Control with Hierarchical Robot Transformers](https://arxiv.org/abs/2410.05273) | --- | --- |
| 2024 | `arXiv` | [Towards Synergistic, Generalized, and Efficient Dual-System for Robotic Manipulation](https://arxiv.org/abs/2410.08001) | --- | [Website](https://robodual.github.io/) |
| 2024 | `CoRL` | [Robotic Control via Embodied Chain-of-Thought Reasoning](https://arxiv.org/abs/2407.08693) | [![Github stars](https://img.shields.io/github/stars/MichalZawalski/embodied-CoT.svg)](https://github.com/MichalZawalski/embodied-CoT) | [Website](https://embodied-cot.github.io/) |




# Survey Paper

<!-- |---|`arXiv`|---|---|---| -->
<!-- [![Github stars](https://img.shields.io/github/stars/***.svg)]() -->
| Year | Venue | Paper Title | Repository | Note |
|:----:|:-----:| ----------- |:----------:|:----:|
|2025|`arXiv`|[Efficient Vision-Language-Action Models for Embodied Manipulation: A Systematic Survey](https://arxiv.org/pdf/2510.17111)|[![Github stars](https://img.shields.io/github/stars/guanweifan/awesome-efficient-vla.svg)](https://github.com/guanweifan/awesome-efficient-vla)|[Blog](https://kwanwaipang.github.io/Efficient-VLA/)|
|2025|`arXiv`|[A Survey on Vision-Language-Action Models: An Action Tokenization Perspective](https://arxiv.org/pdf/2507.01925)|[![Github stars](https://img.shields.io/github/stars/Psi-Robot/Awesome-VLA-Papers.svg)](https://github.com/Psi-Robot/Awesome-VLA-Papers)|---|
|2025|`arXiv`|[Vision-language-action models: Concepts, progress, applications and challenges](https://arxiv.org/pdf/2505.04769?)|---|[Blog](https://kwanwaipang.github.io/VLA-survey-2025/)|
|2025|`arXiv`|[Vision language action models in robotic manipulation: A systematic review](https://arxiv.org/pdf/2507.10672)|---|---|
|2025|`arXiv`|[Large vlm-based vision-language-action models for robotic manipulation: A survey](https://arxiv.org/pdf/2508.13073?)|[![Github stars](https://img.shields.io/github/stars/JiuTian-VL/Large-VLM-based-VLA-for-Robotic-Manipulation.svg)](https://github.com/JiuTian-VL/Large-VLM-based-VLA-for-Robotic-Manipulation)|---|
|2025|`Information Fusion`|[Exploring embodied multimodal large models: Development, datasets, and future directions](https://arxiv.org/pdf/2502.15336?)|---|---|
|2025|`arXiv`|[Pure Vision Language Action (VLA) Models: A Comprehensive Survey](https://arxiv.org/pdf/2509.19012)|---|---|



# Other Resources
* Paper List for [VLN (Visual Language Navigation)](https://github.com/KwanWaiPang/Awesome-VLN)
* [Awesome-Efficient-VLA](https://github.com/guanweifan/awesome-efficient-vla)
* [LeRobot: State-of-the-art Machine Learning for Real-World Robotics in Pytorch / LeRobot: Making AI for Robotics more accessible with end-to-end learning](https://github.com/huggingface/lerobot)



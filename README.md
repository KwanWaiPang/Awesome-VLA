<p align="center">
  <h1 align="center">
  Awesome Visual-Language-Action (VLA)
  </h1>
</p>

This repository contains a curated list of resources addressing the VLA (Visual Language Action).

If you find some ignored papers, **feel free to [*create pull requests*](https://github.com/KwanWaiPang/Awesome-Transformer-based-SLAM/blob/pdf/How-to-PR.md), or [*open issues*](https://github.com/KwanWaiPang/Awesome-VLN/issues/new)**. 

Contributions in any form to make this list more comprehensive are welcome.

If you find this repository useful, a simple star should be the best affirmation. ðŸ˜Š

Feel free to share this list with others!

# Overview
- [Efficient-VLA](#Efficient-VLA)
- [Survey Paper](#Survey-Paper)
- [Other Resources](#Other-Resources)

<br>

<!-- |---|`arXiv`|---|---|---| -->
<!-- [![Github stars](https://img.shields.io/github/stars/***.svg)]() -->
| Year | Venue | Paper Title | Repository | Note |
|:----:|:-----:| ----------- |:----------:|:----:|
|2025|`arXiv`|[Libero-plus: In-depth robustness analysis of vision-language-action models](https://arxiv.org/pdf/2510.13626v1)|[![Github stars](https://img.shields.io/github/stars/sylvestf/LIBERO-plus.svg)](https://github.com/sylvestf/LIBERO-plus)|[website](https://sylvestf.github.io/LIBERO-plus/)|
|2025|`arXiv`|[TraceGen: World Modeling in 3D Trace Space Enables Learning from Cross-Embodiment Videos](https://arxiv.org/pdf/2511.21690)|---|[website](https://tracegen.github.io/)| 
|2025|`arXiv`|[VLA-Pruner: Temporal-Aware Dual-Level Visual Token Pruning for Efficient Vision-Language-Action Inference](https://arxiv.org/pdf/2511.16449)|---|---|
|2025|`arXiv`|[Evo-1: Lightweight Vision-Language-Action Model with Preserved Semantic Alignment](https://arxiv.org/pdf/2511.04555)|[![Github stars](https://img.shields.io/github/stars/MINT-SJTU/Evo-1.svg)](https://github.com/MINT-SJTU/Evo-1)|---|
|2025|`arXiv`|[Spatial Forcing: Implicit Spatial Representation Alignment for Vision-language-action Model](https://arxiv.org/pdf/2510.12276)|[![Github stars](https://img.shields.io/github/stars/OpenHelix-Team/Spatial-Forcing.svg)](https://github.com/OpenHelix-Team/Spatial-Forcing)|[website](https://spatial-forcing.github.io/)|
|2025|`arXiv`|[Evo-0: Vision-language-action model with implicit spatial understanding](https://arxiv.org/pdf/2507.00416?)|[![Github stars](https://img.shields.io/github/stars/MINT-SJTU/Evo-0.svg)](https://github.com/MINT-SJTU/Evo-0)|[website](https://mint-sjtu.github.io/Evo-VLA.io/)|
|2025|`arXiv`|[EvoVLA: Self-Evolving Vision-Language-Action Model](https://arxiv.org/pdf/2511.16166)|[![Github stars](https://img.shields.io/github/stars/AIGeeksGroup/EvoVLA.svg)](https://github.com/AIGeeksGroup/EvoVLA) |[website](https://aigeeksgroup.github.io/EvoVLA/)|
|2025|`arXiv`|[Towards Deploying VLA without Fine-Tuning: Plug-and-Play Inference-Time VLA Policy Steering via Embodied Evolutionary Diffusion](https://arxiv.org/pdf/2511.14178)|---|[website](https://rip4kobe.github.io/vla-pilot/)|
|2025|`arXiv`|[Ï€0.6: a VLA That Learns From Experience](https://www.pi.website/download/pistar06.pdf)|---|[website](https://www.pi.website/blog/pistar06)|
|2025|`ICCV`|[Robomm: All-in-one multimodal large model for robotic manipulation](https://arxiv.org/pdf/2412.07215v1)|[![Github stars](https://img.shields.io/github/stars/EmbodiedAI-RoboTron/RoboTron-Mani.svg)](https://github.com/EmbodiedAI-RoboTron/RoboTron-Mani)|[website](https://robouniview.github.io/RoboMM.github.io/)|
|2025|`arXiv`|[InternVLA-M1: Latent Spatial Grounding for Instruction-Following Robotic Manipulation](https://arxiv.org/pdf/2510.13778)|[![Github stars](https://img.shields.io/github/stars/InternRobotics/InternVLA-M1.svg)](https://github.com/InternRobotics/InternVLA-M1)|[Website](https://internrobotics.github.io/internvla-m1.github.io/)|
|2025|`CoRL`|[Ï€0.5: a Vision-Language-Action Model with Open-World Generalization](https://openreview.net/pdf?id=vlhoswksBO)|[![Github stars](https://img.shields.io/github/stars/Physical-Intelligence/openpi.svg)](https://github.com/Physical-Intelligence/openpi)|[Blog](https://www.pi.website/blog/pi05)<br>PI0.5|
|2025|`CoRR`|[GR00T N1: An Open Foundation Model for Generalist Humanoid Robots](https://arxiv.org/pdf/2503.14734)| [![Github stars](https://img.shields.io/github/stars/NVIDIA/Isaac-GR00T.svg)](https://github.com/NVIDIA/Isaac-GR00T)|[website](https://developer.nvidia.com/isaac/gr00t)|
|2025|`arXiv`|[AnywhereVLA: Language-Conditioned Exploration and Mobile Manipulation](https://arxiv.org/pdf/2509.21006)|[![Github stars](https://img.shields.io/github/stars/SelfAI-research/AnywhereVLA.svg)](https://github.com/SelfAI-research/AnywhereVLA)|[website](https://selfai-research.github.io/AnywhereVLA/)|
|2025|`arXiv`|[Hi robot: Open-ended instruction following with hierarchical vision-language-action models](https://arxiv.org/pdf/2502.19417)|---|[Website](https://www.pi.website/research/hirobot)|
|2025|`arXiv`|[Fast: Efficient action tokenization for vision-language-action models](https://arxiv.org/pdf/2501.09747)|[![Github stars](https://img.shields.io/github/stars/Physical-Intelligence/openpi.svg)](https://github.com/Physical-Intelligence/openpi)|[Website](https://www.pi.website/research/fast)<br>PI0-Fast|
|2025|`CoRL`|[Dexvla: Vision-language model with plug-in diffusion expert for general robot control](https://arxiv.org/pdf/2502.05855)|[![Github stars](https://img.shields.io/github/stars/juruobenruo/DexVLA.svg)](https://github.com/juruobenruo/DexVLA)|[website](https://dex-vla.github.io/)|
|2025|`ICML`|[DiffusionVLA: Scaling Robot Foundation Models via Unified Diffusion and Autoregression](https://openreview.net/pdf?id=VdwdU81Uzy)|---|[website](https://diffusion-vla.github.io/)<br>DiVLA|
|2025|`IROS`|[Agibot world colosseo: A large-scale manipulation platform for scalable and intelligent embodied systems](https://arxiv.org/pdf/2503.06669)|[![Github stars](https://img.shields.io/github/stars/OpenDriveLab/AgiBot-World.svg)](https://github.com/OpenDriveLab/AgiBot-World)|[website](https://agibot-world.com/) <br> GO-1|
|2025|`arXiv`|[Fine-tuning vision-language-action models: Optimizing speed and success](https://arxiv.org/pdf/2502.19645)|[![Github stars](https://img.shields.io/github/stars/moojink/openvla-oft.svg)](https://github.com/moojink/openvla-oft)|[Website](https://openvla-oft.github.io/)<br>OpenVLA-OFT|
|2025|`CoRL`|[OpenVLA: An Open-Source Vision-Language-Action Model](https://arxiv.org/pdf/2406.09246?)|[![Github stars](https://img.shields.io/github/stars/openvla/openvla.svg)](https://github.com/openvla/openvla)|[Website](https://openvla.github.io/)| 
|2025|`ICLR`|[Latent action pretraining from videos](https://arxiv.org/pdf/2410.11758?)|[![Github stars](https://img.shields.io/github/stars/LatentActionPretraining/LAPA.svg)](https://github.com/LatentActionPretraining/LAPA)|[website](https://latentactionpretraining.github.io/)<br>LAPA|
|2024|`RSS`|[Octo: An Open-Source Generalist Robot Policy](https://arxiv.org/pdf/2405.12213)|[![Github stars](https://img.shields.io/github/stars/octo-models/octo.svg)](https://github.com/octo-models/octo)|[Website](https://octo-models.github.io/)|
|2024|`CoRR`|[Ï€0: A Vision-Language-Action Flow Model for General Robot Control](https://arxiv.org/pdf/2410.24164)|[![Github stars](https://img.shields.io/github/stars/Physical-Intelligence/openpi.svg)](https://github.com/Physical-Intelligence/openpi)|[Blog](https://physicalintelligence.company/blog/pi0)<br>PI0|
|2024|`arXiv`|[GR-2: A Generative Video-Language-Action Model with Web-Scale Knowledge for Robot Manipulation](https://arxiv.org/pdf/2410.06158?)|---|[website](https://gr2-manipulation.github.io/)|
|2024|`ICLR`|[Unleashing large-scale video generative pre-training for visual robot manipulation](https://arxiv.org/pdf/2312.13139)|[![Github stars](https://img.shields.io/github/stars/bytedance/GR-1.svg)](https://github.com/bytedance/GR-1)|[website](https://gr1-manipulation.github.io/) <br> GR1|
|2024|`CoRL`|[Rekep: Spatio-temporal reasoning of relational keypoint constraints for robotic manipulation](https://arxiv.org/pdf/2409.01652)|[![Github stars](https://img.shields.io/github/stars/huangwl18/ReKep.svg)](https://github.com/huangwl18/ReKep)|[website](https://rekep-robot.github.io/)|
|2023|`CoRL`|[Voxposer: Composable 3d value maps for robotic manipulation with language models](https://arxiv.org/pdf/2307.05973)|[![Github stars](https://img.shields.io/github/stars/huangwl18/VoxPoser.svg)](https://github.com/huangwl18/VoxPoser)|[website](https://voxposer.github.io/)|
|2023|`CoRL`|[Rt-2: Vision-language-action models transfer web knowledge to robotic control](https://robotics-transformer2.github.io/assets/rt2.pdf)|---|[Website](https://robotics-transformer2.github.io/)|
|2023|`RSS`|[Learning fine-grained bimanual manipulation with low-cost hardware](https://arxiv.org/pdf/2304.13705)|[![Github stars](https://img.shields.io/github/stars/tonyzhaozh/act.svg)](https://github.com/tonyzhaozh/act)|[Website](https://tonyzhaozh.github.io/aloha/)<br>ALOHA/ACT|
|2023|`CoRL`|[Do as i can, not as i say: Grounding language in robotic affordances](https://proceedings.mlr.press/v205/ichter23a/ichter23a.pdf)|---|[website](https://say-can.github.io/)<br>SayCan|
|2022|`arXiv`|[Rt-1: Robotics transformer for real-world control at scale](https://arxiv.org/pdf/2212.06817)|[![Github stars](https://img.shields.io/github/stars/google-research/robotics_transformer.svg)](https://github.com/google-research/robotics_transformer)|[website](https://robotics-transformer1.github.io/)<br>[Blog](https://research.google/blog/rt-1-robotics-transformer-for-real-world-control-at-scale/)|




# Efficient-VLA

<!-- |---|`arXiv`|---|---|---| -->
<!-- [![Github stars](https://img.shields.io/github/stars/***.svg)]() -->
| Year | Venue | Paper Title | Repository | Note |
|:----:|:-----:| ----------- |:----------:|:----:|
|2025|`arXiv`|[Running VLAs at Real-time Speed](https://arxiv.org/pdf/2510.26742)|[![Github stars](https://img.shields.io/github/stars/Dexmal/realtime-vla.svg)](https://github.com/Dexmal/realtime-vla)|[Blog](https://kwanwaipang.github.io/realtime-vla/)|
|2025|`arXiv`|[NanoVLA: Routing Decoupled Vision-Language Understanding for Nano-sized Generalist Robotic Policies](https://arxiv.org/pdf/2510.25122v1)|---|---|
|2025|`arXiv`|[Action-aware dynamic pruning for efficient vision-language-action manipulation](https://arxiv.org/pdf/2509.22093?)|---|---|
|2025|`arXiv`|[Edgevla: Efficient vision-language-action models](https://arxiv.org/pdf/2507.14049)|---|---|
|2025|`arXiv`|[Hume: Introducing System-2 Thinking in Visual-Language-Action Model](https://arxiv.org/pdf/2505.21432)|[![Github stars](https://img.shields.io/github/stars/hume-vla/hume.svg)](https://github.com/hume-vla/hume)|[website](https://hume-vla.github.io/)|
|2025|`CoRL`|[Flower: Democratizing generalist robot policies with efficient vision-language-action flow policies](https://arxiv.org/pdf/2509.04996)|[![Github stars](https://img.shields.io/github/stars/intuitive-robots/flower_vla_pret.svg)](https://github.com/intuitive-robots/flower_vla_pret) <br> [![Github stars](https://img.shields.io/github/stars/intuitive-robots/flower_vla_calvin.svg)](https://github.com/intuitive-robots/flower_vla_calvin)|[website](https://intuitive-robots.github.io/flower_vla/)|
|2025|`arXiv`|[Accelerating vision-language-action model integrated with action chunking via parallel decoding](https://arxiv.org/pdf/2503.02310)|---|---|
|2025|`arXiv`|[Fine-tuning vision-language-action models: Optimizing speed and success](https://arxiv.org/pdf/2502.19645)|[![Github stars](https://img.shields.io/github/stars/moojink/openvla-oft.svg)](https://github.com/moojink/openvla-oft)|[website](https://openvla-oft.github.io/)|
|2025|`arXiv`|[Nina: Normalizing flows in action. training vla models with normalizing flows](https://arxiv.org/pdf/2508.16845)|[![Github stars](https://img.shields.io/github/stars/dunnolab/NinA.svg)](https://github.com/dunnolab/NinA/)|---|
|2025|`ICCV`|[Saliency-aware quantized imitation learning for efficient robotic control](https://openaccess.thecvf.com/content/ICCV2025/papers/Park_Saliency-Aware_Quantized_Imitation_Learning_for_Efficient_Robotic_Control_ICCV_2025_paper.pdf)|---|---|
|2025|`arXiv`|[Don't Run with Scissors: Pruning Breaks VLA Models but They Can Be Recovered](https://arxiv.org/pdf/2510.08464)|---|[website](https://gluestick-vla.github.io/)|
|2025|`arXiv`|[VITA-VLA: Efficiently Teaching Vision-Language Models to Act via Action Expert Distillation](https://arxiv.org/pdf/2510.09607)|[![Github stars](https://img.shields.io/github/stars/Tencent/VITA.svg)](https://github.com/Tencent/VITA)|[website](https://ltbai.github.io/VITA-VLA/)|
|2025|`arXiv`|[CEED-VLA: Consistency Vision-Language-Action Model with Early-Exit Decoding](https://arxiv.org/pdf/2506.13725)|[![Github stars](https://img.shields.io/github/stars/OpenHelix-Team/CEED-VLA.svg)](https://github.com/OpenHelix-Team/CEED-VLA)|[website](https://irpn-eai.github.io/CEED-VLA/)|
|2025|`arXiv`|[Omnisat: Compact action token, faster auto regression](https://arxiv.org/pdf/2510.09667)|---|---|
|2025|`arXiv`|[RetoVLA: Reusing Register Tokens for Spatial Reasoning in Vision-Language-Action Models](https://arxiv.org/pdf/2509.21243)|---|[demo](youtu.be/2CseBR-snZg)|
|2025|`arXiv`|[KV-Efficient VLA: A Method of Speed up Vision Language Model with RNN-Gated Chunked KV Cache](https://arxiv.org/pdf/2509.21354)|[![Github stars](https://img.shields.io/github/stars/l-zhuang/KV-Efficient-VLA.svg)](https://github.com/l-zhuang/KV-Efficient-VLA)|---|
|2025|`arXiv`|[Fast ECoT: Efficient Embodied Chain-of-Thought via Thoughts Reuse](https://arxiv.org/pdf/2506.07639)|---|---|
|2025|`arXiv`|[Ttf-vla: Temporal token fusion via pixel-attention integration for vision-language-action models](https://arxiv.org/pdf/2508.19257)|---|---|
|2025|`arXiv`|[Unified Vision-Language-Action Model](https://arxiv.org/pdf/2506.19850)|[![Github stars](https://img.shields.io/github/stars/baaivision/UniVLA.svg)](https://github.com/baaivision/UniVLA) |[website](https://robertwyq.github.io/univla.github.io/)|
|2025|`ICML`|[Otter: A vision-language-action model with text-aware visual feature extraction](https://arxiv.org/pdf/2503.03734)|[![Github stars](https://img.shields.io/github/stars/Max-Fu/otter.svg)](https://github.com/Max-Fu/otter)|[Website](https://ottervla.github.io/)|
|2025|`arXiv`|[Sqap-vla: A synergistic quantization-aware pruning framework for high-performance vision-language-action models](https://arxiv.org/pdf/2509.09090?)|[![Github stars](https://img.shields.io/github/stars/ecdine/SQAP-VLA.svg)](https://github.com/ecdine/SQAP-VLA)|---|
|2025|`arXiv`|[Fastdrivevla: Efficient end-to-end driving via plug-and-play reconstruction-based token pruning](https://arxiv.org/pdf/2507.23318)|---|---|
| 2025 | `RAL` | [TinyVLA: Towards Fast, Data-Efficient Vision-Language-Action Models for Robotic Manipulation](https://arxiv.org/abs/2409.12514) | [![Github stars](https://img.shields.io/github/stars/liyaxuanliyaxuan/TinyVLA.svg)](https://github.com/liyaxuanliyaxuan/TinyVLA) | [Website](https://tiny-vla.github.io/) |
|2025|`arXiv`|[Smolvla: A vision-language-action model for affordable and efficient robotics](https://arxiv.org/pdf/2506.01844)|---|[website](https://huggingface.co/blog/smolvla)|
| 2025 | `arXiv` | [NORA: A Small Open-Sourced Generalist Vision-Language-Action Model for Embodied Tasks](https://arxiv.org/abs/2504.19854) | [![Github stars](https://img.shields.io/github/stars/declare-lab/nora.svg)](https://github.com/declare-lab/nora) | [Website](https://declare-lab.github.io/nora) |
| 2025 | `arXiv` | [MoLe-VLA: Dynamic Layer-Skipping Vision-Language-Action Model via Mixture-of-Layers for Efficient Robot Manipulation](https://arxiv.org/abs/2503.20384) | [![Github stars](https://img.shields.io/github/stars/RoyZry98/MoLe-VLA-Pytorch.svg)](https://github.com/RoyZry98/MoLe-VLA-Pytorch) | [Website](https://sites.google.com/view/mole-vla) |
| 2025 | `arXiv` | [EfficientVLA: Trainingâ€‘Free Acceleration and Compression for Visionâ€‘Languageâ€‘Action Models](https://arxiv.org/abs/2506.10100) | --- | --- |
| 2025 | `arXiv` | [Think Twice, Act Once: Tokenâ€‘Aware Compression and Action Reuse for Efficient Inference in Visionâ€‘Languageâ€‘Action Models](https://arxiv.org/abs/2505.21200) | --- | --- |
| 2025 | `arXiv` | [ThinkAct: Vision-Language-Action Reasoning via Reinforced Visual Latent Planning](https://arxiv.org/abs/2507.16815) | --- | [Website](https://jasper0314-huang.github.io/thinkact-vla/) |
| 2025 | `arXiv` | [OpenHelix: A Short Survey, Empirical Analysis, and Open-Source Dual-System VLA Model for Robotic Manipulation](https://arxiv.org/abs/2505.03912) | [![Github stars](https://img.shields.io/github/stars/OpenHelix-robot/OpenHelix.svg)](https://github.com/OpenHelix-robot/OpenHelix) | [Website](https://openhelix-robot.github.io/) |
| 2025 | `arXiv` | [SpatialVLA: Exploring Spatial Representations for Visual-Language-Action Model](https://arxiv.org/abs/2501.15830) | [![Github stars](https://img.shields.io/github/stars/SpatialVLA/SpatialVLA.svg)](https://github.com/SpatialVLA/SpatialVLA) | [Website](https://spatialvla.github.io/) |
| 2025 | `arXiv` | [Fast-in-Slow: A Dual-System Foundation Model Unifying Fast Manipulation within Slow Reasoning](https://arxiv.org/abs/2506.01953) | [![Github stars](https://img.shields.io/github/stars/CHEN-H01/Fast-in-Slow.svg)](https://github.com/CHEN-H01/Fast-in-Slow) | [Website](https://fast-in-slow.github.io/) |
| 2025 | `arXiv` | [SP-VLA: A Joint Model Scheduling and Token Pruning Approach for VLA Model Acceleration](https://arxiv.org/abs/2506.12723) | --- | --- |
| 2025 | `arXiv` | [VLAâ€‘Cache: Towards Efficient Visionâ€‘Languageâ€‘Action Model via Adaptive Token Caching in Robotic Manipulation](https://arxiv.org/abs/2502.02175) | [![Github stars](https://img.shields.io/github/stars/siyuhsu/vla-cache.svg)](https://github.com/siyuhsu/vla-cache) | --- |
| 2025 | `arXiv` | [SpecPrune-VLA: Accelerating Vision-Language-Action Models via Action-Aware Self-Speculative Pruning](https://arxiv.org/abs/2509.05614) | --- | --- |
| 2025 | `RSS` | [FAST: Efficient Action Tokenization for Vision-Language-Action Models](https://www.roboticsproceedings.org/rss21/p012.pdf) | --- | [Website](https://www.pi.website/research/fast) |
| 2025 | `arXiv` | [Real-Time Execution of Action Chunking Flow Policies](https://arxiv.org/abs/2506.07339) | --- | [Website](https://www.pi.website/research/real_time_chunking) |
| 2025 | `arXiv` | [VOTE: Vision-Language-Action Optimization with Trajectory Ensemble Voting](https://arxiv.org/abs/2507.05116) | --- | --- |
| 2025 | `arXiv` | [BitVLA: 1-Bit Vision-Language-Action Models for Robotics Manipulation](https://arxiv.org/abs/2506.07530) | [![Github stars](https://img.shields.io/github/stars/ustcwhy/BitVLA.svg)](https://github.com/ustcwhy/BitVLA) | --- |
| 2025 | `arXiv` | [SpecVLA: Speculative Decoding for Vision-Language-Action Models with Relaxed Acceptance](https://arxiv.org/abs/2507.22424) | --- | --- |
| 2024 | `NeurIPS` | [RoboMamba: Efficient Vision-Language-Action Model for Robotic Reasoning and Manipulation](https://arxiv.org/abs/2406.04339) | [![Github stars](https://img.shields.io/github/stars/lmzpai/roboMamba.svg)](https://github.com/lmzpai/roboMamba) | [Website](https://sites.google.com/view/robomamba-web) |
| 2024 | `NeurIPS` | [DeeR-VLA: Dynamic Inference of Multimodal Large Language Models for Efficient Robot Execution](https://arxiv.org/abs/2411.02359) | [![Github stars](https://img.shields.io/github/stars/yueyang130/DeeR-VLA.svg)](https://github.com/yueyang130/DeeR-VLA) | --- |
| 2024 | `IROS` | [From LLMs to Actions: Latent Codes as Bridges in Hierarchical Robot Control](https://arxiv.org/abs/2405.04798) | --- | --- |
| 2024 | `CoRL` | [HIRT: Enhancing Robotic Control with Hierarchical Robot Transformers](https://arxiv.org/abs/2410.05273) | --- | --- |
| 2024 | `arXiv` | [Towards Synergistic, Generalized, and Efficient Dual-System for Robotic Manipulation](https://arxiv.org/abs/2410.08001) | --- | [Website](https://robodual.github.io/) |
| 2024 | `CoRL` | [Robotic Control via Embodied Chain-of-Thought Reasoning](https://arxiv.org/abs/2407.08693) | [![Github stars](https://img.shields.io/github/stars/MichalZawalski/embodied-CoT.svg)](https://github.com/MichalZawalski/embodied-CoT) | [Website](https://embodied-cot.github.io/) |




# Survey Paper

<!-- |---|`arXiv`|---|---|---| -->
<!-- [![Github stars](https://img.shields.io/github/stars/***.svg)]() -->
| Year | Venue | Paper Title | Repository | Note |
|:----:|:-----:| ----------- |:----------:|:----:|
|2025|`arXiv`|[Efficient Vision-Language-Action Models for Embodied Manipulation: A Systematic Survey](https://arxiv.org/pdf/2510.17111)|[![Github stars](https://img.shields.io/github/stars/guanweifan/awesome-efficient-vla.svg)](https://github.com/guanweifan/awesome-efficient-vla)|[Blog](https://kwanwaipang.github.io/Efficient-VLA/)|
|2025|`arXiv`|[A Survey on Vision-Language-Action Models: An Action Tokenization Perspective](https://arxiv.org/pdf/2507.01925)|[![Github stars](https://img.shields.io/github/stars/Psi-Robot/Awesome-VLA-Papers.svg)](https://github.com/Psi-Robot/Awesome-VLA-Papers)|---|
|2025|`arXiv`|[Vision-language-action models: Concepts, progress, applications and challenges](https://arxiv.org/pdf/2505.04769?)|---|[Blog](https://kwanwaipang.github.io/VLA-survey-2025/)|
|2025|`arXiv`|[Vision language action models in robotic manipulation: A systematic review](https://arxiv.org/pdf/2507.10672)|---|---|
|2025|`arXiv`|[Large vlm-based vision-language-action models for robotic manipulation: A survey](https://arxiv.org/pdf/2508.13073?)|[![Github stars](https://img.shields.io/github/stars/JiuTian-VL/Large-VLM-based-VLA-for-Robotic-Manipulation.svg)](https://github.com/JiuTian-VL/Large-VLM-based-VLA-for-Robotic-Manipulation)|---|
|2025|`Information Fusion`|[Exploring embodied multimodal large models: Development, datasets, and future directions](https://arxiv.org/pdf/2502.15336?)|---|---|
|2025|`arXiv`|[Pure Vision Language Action (VLA) Models: A Comprehensive Survey](https://arxiv.org/pdf/2509.19012)|---|---|



# Other Resources
* Paper List for [VLN (Visual Language Navigation)](https://github.com/KwanWaiPang/Awesome-VLN)
* [Awesome-Efficient-VLA](https://github.com/guanweifan/awesome-efficient-vla)
* [LeRobot: State-of-the-art Machine Learning for Real-World Robotics in Pytorch / LeRobot: Making AI for Robotics more accessible with end-to-end learning](https://github.com/huggingface/lerobot)



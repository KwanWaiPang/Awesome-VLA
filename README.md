<p align="center">
  <h1 align="center">
  Awesome Visual-Language-Action (VLA)
  </h1>
</p>

This repository contains a curated list of resources addressing the VLA (Visual Language Action).

If you find some ignored papers, **feel free to [*create pull requests*](https://github.com/KwanWaiPang/Awesome-Transformer-based-SLAM/blob/pdf/How-to-PR.md), or [*open issues*](https://github.com/KwanWaiPang/Awesome-VLN/issues/new)**. 

Contributions in any form to make this list more comprehensive are welcome.

If you find this repository useful, a simple star should be the best affirmation. üòä

Feel free to share this list with others!

# Overview
- [Survey Paper](#Survey-Paper)
- [Efficient-VLA](#Efficient-VLA)
- [Other Resources](#Others-Resources)

<br>

<!-- |---|`arXiv`|---|---|---| -->
<!-- [![Github stars](https://img.shields.io/github/stars/***.svg)]() -->
| Year | Venue | Paper Title | Repository | Note |
|:----:|:-----:| ----------- |:----------:|:----:|
|2025|`arXiv`|[Smolvla: A vision-language-action model for affordable and efficient robotics](https://arxiv.org/pdf/2506.01844)|---|---|
|2025|`CoRL`|[œÄ0.5: a Vision-Language-Action Model with Open-World Generalization](https://openreview.net/pdf?id=vlhoswksBO)|[![Github stars](https://img.shields.io/github/stars/Physical-Intelligence/openpi.svg)](https://github.com/Physical-Intelligence/openpi)|[Blog](https://www.pi.website/blog/pi05)<br>PI0.5|
|2025|`arXiv`|[Pure Vision Language Action (VLA) Models: A Comprehensive Survey](https://arxiv.org/pdf/2509.19012)|---|---|
|2025|`arXiv`|[InternVLA-M1: Latent Spatial Grounding for Instruction-Following Robotic Manipulation](https://arxiv.org/pdf/2510.13778)|[![Github stars](https://img.shields.io/github/stars/InternRobotics/InternVLA-M1.svg)](https://github.com/InternRobotics/InternVLA-M1)|[Website](https://internrobotics.github.io/internvla-m1.github.io/)|
|2025|`arXiv`|[AnywhereVLA: Language-Conditioned Exploration and Mobile Manipulation](https://arxiv.org/pdf/2509.21006)|[![Github stars](https://img.shields.io/github/stars/SelfAI-research/AnywhereVLA.svg)](https://github.com/SelfAI-research/AnywhereVLA)|[website](https://selfai-research.github.io/AnywhereVLA/)|
|2025|`arXiv`|[Hi robot: Open-ended instruction following with hierarchical vision-language-action models](https://arxiv.org/pdf/2502.19417)|---|[Website](https://www.pi.website/research/hirobot)|
|2025|`arXiv`|[Fast: Efficient action tokenization for vision-language-action models](https://arxiv.org/pdf/2501.09747)|[![Github stars](https://img.shields.io/github/stars/Physical-Intelligence/openpi.svg)](https://github.com/Physical-Intelligence/openpi)|[Website](https://www.pi.website/research/fast)<br>PI0-Fast|
|2025|`arXiv`|[Agibot world colosseo: A large-scale manipulation platform for scalable and intelligent embodied systems](https://arxiv.org/pdf/2503.06669)|[![Github stars](https://img.shields.io/github/stars/OpenDriveLab/AgiBot-World.svg)](https://github.com/OpenDriveLab/AgiBot-World)|---|
|2025|`CoRL`|[OpenVLA: An Open-Source Vision-Language-Action Model](https://arxiv.org/pdf/2406.09246?)|[![Github stars](https://img.shields.io/github/stars/openvla/openvla.svg)](https://github.com/openvla/openvla)|[Website](https://openvla.github.io/)| 
|2024|`RSS`|[Octo: An Open-Source Generalist Robot Policy](https://arxiv.org/pdf/2405.12213)|[![Github stars](https://img.shields.io/github/stars/octo-models/octo.svg)](https://github.com/octo-models/octo)|[Website](https://octo-models.github.io/)|
|2024|`CoRR`|[œÄ0: A Vision-Language-Action Flow Model for General Robot Control](https://arxiv.org/pdf/2410.24164)|[![Github stars](https://img.shields.io/github/stars/Physical-Intelligence/openpi.svg)](https://github.com/Physical-Intelligence/openpi)|[Blog](https://physicalintelligence.company/blog/pi0)<br>PI0|
|2023|`CoRL`|[Rt-2: Vision-language-action models transfer web knowledge to robotic control](https://robotics-transformer2.github.io/assets/rt2.pdf)|---|[Website](https://robotics-transformer2.github.io/)|
|2023|`RSS`|[Learning fine-grained bimanual manipulation with low-cost hardware](https://arxiv.org/pdf/2304.13705)|[![Github stars](https://img.shields.io/github/stars/tonyzhaozh/act.svg)](https://github.com/tonyzhaozh/act)|[Website](https://tonyzhaozh.github.io/aloha/)<br>ALOHA/ACT|
|2022|`arXiv`|[Rt-1: Robotics transformer for real-world control at scale](https://arxiv.org/pdf/2212.06817)|[![Github stars](https://img.shields.io/github/stars/google-research/robotics_transformer.svg)](https://github.com/google-research/robotics_transformer)|[website](https://robotics-transformer1.github.io/)<br>[Blog](https://research.google/blog/rt-1-robotics-transformer-for-real-world-control-at-scale/)|


# Survey Paper

<!-- |---|`arXiv`|---|---|---| -->
<!-- [![Github stars](https://img.shields.io/github/stars/***.svg)]() -->
| Year | Venue | Paper Title | Repository | Note |
|:----:|:-----:| ----------- |:----------:|:----:|
|2025|`arXiv`|[Efficient Vision-Language-Action Models for Embodied Manipulation: A Systematic Survey](https://arxiv.org/pdf/2510.17111)|[![Github stars](https://img.shields.io/github/stars/guanweifan/awesome-efficient-vla.svg)](https://github.com/guanweifan/awesome-efficient-vla)|[Blog](https://kwanwaipang.github.io/Efficient-VLA/)|
|2025|`arXiv`|[A Survey on Vision-Language-Action Models: An Action Tokenization Perspective](https://arxiv.org/pdf/2507.01925)|[![Github stars](https://img.shields.io/github/stars/Psi-Robot/Awesome-VLA-Papers.svg)](https://github.com/Psi-Robot/Awesome-VLA-Papers)|---|
|2025|`arXiv`|[Vision-language-action models: Concepts, progress, applications and challenges](https://arxiv.org/pdf/2505.04769?)|---|[Blog](https://kwanwaipang.github.io/VLA-survey-2025/)|
|2025|`arXiv`|[Vision language action models in robotic manipulation: A systematic review](https://arxiv.org/pdf/2507.10672)|---|---|
|2025|`arXiv`|[Large vlm-based vision-language-action models for robotic manipulation: A survey](https://arxiv.org/pdf/2508.13073?)|[![Github stars](https://img.shields.io/github/stars/JiuTian-VL/Large-VLM-based-VLA-for-Robotic-Manipulation.svg)](https://github.com/JiuTian-VL/Large-VLM-based-VLA-for-Robotic-Manipulation)|---|
|2025|`Information Fusion`|[Exploring embodied multimodal large models: Development, datasets, and future directions](https://arxiv.org/pdf/2502.15336?)|---|---|
|2025|`arXiv`|[Pure Vision Language Action (VLA) Models: A Comprehensive Survey](https://arxiv.org/pdf/2509.19012)|---|---|




# Efficient-VLA

<!-- |---|`arXiv`|---|---|---| -->
<!-- [![Github stars](https://img.shields.io/github/stars/***.svg)]() -->
| Year | Venue | Paper Title | Repository | Note |
|:----:|:-----:| ----------- |:----------:|:----:|
|2025|`arXiv`|[Running VLAs at Real-time Speed](https://arxiv.org/pdf/2510.26742)|[![Github stars](https://img.shields.io/github/stars/Dexmal/realtime-vla.svg)](https://github.com/Dexmal/realtime-vla)|[Blog](https://kwanwaipang.github.io/realtime-vla/)|
|2025|`arXiv`|[NanoVLA: Routing Decoupled Vision-Language Understanding for Nano-sized Generalist Robotic Policies](https://arxiv.org/pdf/2510.25122v1)|---|---|
| 2025 | RA-L | [Tinyvla: towards fast, data-efficient vision-language-action models for robotic manipulation](https://arxiv.org/abs/2409.12514) | [üåê](https://tiny-vla.github.io/) | [üíª](https://github.com/liyaxuanliyaxuan/TinyVLA) |
| 2025 | arXiv | [Smolvla: a vision-language-action model for affordable and efficient robotics](https://arxiv.org/abs/2506.01844) | - | [üíª](https://github.com/huggingface/lerobot) |
| 2025 | arXiv | [NORA: A Small Open-Sourced Generalist Vision-Language-Action Model for Embodied Tasks](https://arxiv.org/abs/2504.19854) | [üåê](https://declare-lab.github.io/nora) | [üíª](https://github.com/declare-lab/nora) |
| 2025 | arXiv | [MoLe-VLA: Dynamic Layer-Skipping Vision-Language-Action Model via Mixture-of-Layers for Efficient Robot Manipulation](https://arxiv.org/abs/2503.20384) | [üåê](https://sites.google.com/view/mole-vla) | [üíª](https://github.com/RoyZry98/MoLe-VLA-Pytorch) |
| 2025 | arXiv | [EfficientVLA: Training‚ÄëFree Acceleration and Compression for Vision‚ÄëLanguage‚ÄëAction Models](https://arxiv.org/abs/2506.10100) | - | - |
| 2025 | arXiv | [Openhelix: A short survey, empirical analysis, and open-source dual-system vla model for robotic manipulation](https://arxiv.org/abs/2505.03912) | [üåê](https://openhelix-robot.github.io/) | [üíª](https://github.com/OpenHelix-robot/OpenHelix) 
| 2025 | arXiv | [Fast-in-slow: a dual-system foundation model unifying fast manipulation within slow reasoning](https://arxiv.org/abs/2506.01953) | [üåê](https://fast-in-slow.github.io/) | [üíª](https://github.com/CHEN-H01/Fast-in-Slow) |
| 2024 | NeurIPS | [RoboMamba: Efficient Vision-Language-Action Model for Robotic Reasoning and Manipulation](https://arxiv.org/abs/2406.04339) | [üåê](https://sites.google.com/view/robomamba-web) | [üíª](https://github.com/lmzpai/roboMamba?tab=readme-ov-file) |
| 2024 | NeurIPS | [DeeR-VLA: Dynamic Inference of Multimodal Large Language Models for Efficient Robot Execution](https://arxiv.org/abs/2411.02359) | [üåê](-) | [üíª](https://github.com/yueyang130/DeeR-VLA) |
| 2024 | IROS | [From llms to actions: latent codes as bridges in hierarchical robot control](https://arxiv.org/abs/2405.04798) | - | - |
| 2024 | CoRL | [Hirt: enhancing robotic control with hierarchical robot transformers](https://arxiv.org/abs/2410.05273) | - | - |
| 2024 | arXiv | [Towards synergistic, generalized, and efficient dual-system for robotic manipulation](https://arxiv.org/abs/2410.08001) | [üåê](https://robodual.github.io/) | - |





# Other Resources
* Paper List for [VLN (Visual Language Navigation)](https://github.com/KwanWaiPang/Awesome-VLN)



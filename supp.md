## VLAæ–¹æ³•æ€»è§ˆ

<!-- |  å¹´ä»½ |  å•ä½  | æ¨¡å‹  |  æ–¹æ³•  | è¯´æ˜ | -->

|  å¹´ä»½ |  å•ä½  | æ¨¡å‹  |  æ–¹æ³•  | è¯´æ˜ |
|:-----:|:-----:|:-----:|:-----:| ---------- |
|  2025 |  ä¸Šæµ·äº¤é€šå¤§å­¦  | [Mantis](https://arxiv.org/pdf/2511.16175)  |  æ–¹æ³•  | å¼ºè°ƒå¯¹VLAæ¨¡å‹çš„è¯­è¨€ç›‘ç£(å¼•å…¥è¯­è¨€ç”Ÿæˆï¼Œä¿ç•™VLMä¸»å¹²çš„ç†è§£ä¸æ¨ç†èƒ½åŠ›)ï¼Œè§†è§‰é¢„æµ‹ä¸åŠ¨ä½œç”Ÿæˆè§£è€¦(å‡è½»ä¸»ç½‘ç»œè´Ÿæ‹…ï¼Œæå‡fine-tuneçš„æ”¶æ•›)ï¼ŒLIBERO benchmarkä¸Šå¹³å‡SRè¾¾96.7% |
|  2025 |  æ¸…åå¤§å­¦  | [Motus](https://arxiv.org/pdf/2512.13030)  |  VLM(Qwen3-VL-2B)+World Model(Wan2.2-5B)+action expert(Transformer)  | VLAå’Œä¸–ç•Œæ¨¡å‹çº³å…¥äº†ç»Ÿä¸€æ¡†æ¶;ç ”ç©¶åŸºäº UniDiffuserï¼ˆç»Ÿä¸€æ‰©æ•£ç”Ÿæˆæ¡†æ¶ï¼‰çš„å»ºæ¨¡ç†è®ºï¼Œå°†â€œè§†é¢‘ç”Ÿæˆâ€å’Œâ€œåŠ¨ä½œç”Ÿæˆâ€è§†ä¸ºåŒä¸€ç”Ÿæˆè¿‡ç¨‹ä¸­çš„ä¸¤ä¸ªå˜é‡ï¼Œå¹¶ä¸ºäºŒè€…è®¾è®¡äº†ç›¸äº’ç‹¬ç«‹çš„å™ªå£°è°ƒåº¦æœºåˆ¶;é‡‡ç”¨äº† MoTï¼ˆMixture-of-Transformersï¼Œä¸“å®¶æ··åˆ Transformerï¼‰ ç»“æ„ï¼Œå°†æ“…é•¿ç†è§£è¯­ä¹‰çš„è§†è§‰è¯­è¨€æ¨¡å‹å’Œæ“…é•¿å»ºæ¨¡åŠ¨æ€çš„ç”Ÿæˆå¼è§†é¢‘æ¨¡å‹ä½œä¸ºä¸åŒâ€œä¸“å®¶â€å¼•å…¥åŒä¸€æ¡†æ¶;RoboTwin2.0 50ä¸ªä»»åŠ¡å¹³å‡æˆåŠŸç‡ä¸º87ï½88%ï¼ˆæ¯”PI0.5é«˜45%ï¼Œæ¯”X-VLAé«˜15%ï¼‰ï¼ŒçœŸæœºéªŒè¯ä¹Ÿé«˜äºPI0.5 |
|  2025 |  åŒ—äº¬äººå½¢æœºå™¨äºº  | [XR-1](https://arxiv.org/pdf/2511.02776v1) |  VLMï¼ˆåŒåˆ†æ”¯çš„å‘é‡åŒ–è‡ªç¼–ç å™¨ï¼ŒPaliGemma+Gemma+action headï¼‰  | Unified Vision-Motion Codes (UVMCï¼Œå¤šæ¨¡æ€ç»Ÿä¸€è§†è§‰-åŠ¨ä½œç¼–ç ï¼‰å°†è§†è§‰è§‚å¯Ÿã€è¯­è¨€æŒ‡ä»¤å’Œæœºå™¨äººåŠ¨ä½œåœ¨ç»Ÿä¸€çš„è¡¨å¾ç©ºé—´ä¸­è¿›è¡Œå­¦ä¹ ï¼›ä¸‰é˜¶æ®µè®­ç»ƒï¼šå¤šæ¨¡æ€é¢„è®­ç»ƒâ†’è·¨æœ¬ä½“ä¸»ç½‘ç»œè®­ç»ƒâ†’ç‰¹å®šåœºæ™¯å¾®è°ƒï¼›å¤§å‹æ•°æ®é›†RoboMIND V2.0ï¼ˆçœŸå®æ•°æ®+ArtVIPä»¿çœŸæ•°æ®ï¼‰ï¼›åœ¨å¤©å·¥2.0äººå½¢æœºå™¨äººã€URã€Frankaç­‰æœ¬ä½“ä¸ŠéªŒè¯ |
|  2025 |  æä½³ç§‘æŠ€  | [SwiftVLA](https://arxiv.org/pdf/2512.00903)  |  VLM  |  4D transformerï¼ˆStreamVGGTï¼‰å®ç°ä»2Då›¾åƒæå–4Dç‰¹å¾ï¼ˆ2D+4Dè¾“å…¥ï¼‰ï¼Œä»¥æ­¤æ¥å¢å¼ºVLMï¼ˆSmolVLM-0.5Bï¼‰çš„ç‰¹å¾ï¼ŒçœŸæœºå®éªŒæ¯”7å€æ›´å¤§çš„VLAæ¨¡å‹ï¼ˆÏ€0ï¼‰æ€§èƒ½ç›¸å½“ï¼ˆçœŸæœºæˆåŠŸç‡ä¸º0.76ï¼ŒÏ€0ä¸º 0.48ï¼ŒLIBEROæˆåŠŸç‡ä¸º95.1%ï¼‰ï¼Œåœ¨è¾¹ç«¯è®¾å¤‡ï¼ˆNVIDIA Jetson Orinï¼‰ä¸Šå¿«18å€ï¼ˆæ¨ç†é€Ÿåº¦ä¸º0.167s=5.98HZï¼‰ï¼Œå†…å­˜æ¶ˆè€—å°‘12å€ |
|  2025 |  å¤æ—¦å¤§å­¦  | [ProphRL](https://arxiv.org/pdf/2511.20633)  |  VLA+RL+World Model  | æå‡ºäº†ä¸€ä¸ªä¸–ç•Œæ¨¡å‹ï¼ˆProphetï¼‰ä½œä¸ºæ¨¡æ‹Ÿå™¨æ¥è®­ç»ƒVLAï¼Œæ–¹æ³•é‡‡ç”¨VLA+åœ¨çº¿RLï¼›åœ¨AgiBotã€DROIDã€LIBERO å’Œ BRIDGE ç­‰benchmarkä¸Šä¸ºå„ç±» VLA æ¨¡å‹ï¼ˆVLA-adapter-0.5B, Pi0.5-3B,  OpenVLA-OFT-7Bï¼‰å¸¦æ¥ 5â€“17% çš„SRæå‡ï¼Œåœ¨çœŸå®æœºå™¨ä¸­å–å¾— 24â€“30% çš„æˆåŠŸç‡æå‡ | 
|  2025 |  å¤æ—¦å¤§å­¦  | [WholeBodyVLA](https://arxiv.org/pdf/2512.11047)  |  VLM+RL | ç»Ÿä¸€æ¡†æ¶å®ç°å…¨èº«æ§åˆ¶ï¼ˆä¸‹è‚¢ç§»åŠ¨+ä¸Šè‚¢æ‰å–ï¼‰ã€‚LAM ï¼ˆLatent Action Modelï¼‰é‡‡ç”¨åˆ†æ®µè®­ç»ƒçš„æ–¹å¼ï¼Œç”¨çœŸå®æœºå™¨äººæ“ä½œæ•°æ®é›†ï¼ˆAgiBot Worldï¼‰+å¸¦æœ‰â€œç§»åŠ¨-æ“ä½œâ€çš„è§†é¢‘è¿›è¡Œé¢„è®­ç»ƒï¼Œå­¦ä¹ locomotionâ€“manipulation skillã€‚è€Œlocoâ€“manipulationâ€“oriented (LMO) RL policyåˆ™ç”¨äºå®ç°ç¨³å®šçš„ç§»åŠ¨ï¼ˆå¦‚ï¼Œå‰è¿›ã€è½¬å¼¯ã€ä¸‹è¹²ï¼‰ |
|  2025 |  é˜¿é‡Œè¾¾æ‘©é™¢  | [RynnVLA-002](https://arxiv.org/pdf/2511.17502)  |  Action World Model  | ç»Ÿä¸€æ¡†æ¶å®ç°ï¼ˆVLA+World Modelï¼‰ï¼›â€œåŒå‘å¢å¼ºâ€ é€»è¾‘ï¼šä¸–ç•Œæ¨¡å‹é€šè¿‡å­¦ä¹ ç‰©ç†åŠ¨æ€ï¼ˆé¢„æµ‹/ç”Ÿæˆæœªæ¥å›¾åƒï¼‰ï¼Œå¯ä¼˜åŒ– VLA æ¨¡å‹çš„åŠ¨ä½œç”Ÿæˆç²¾åº¦ï¼›è€Œ VLA æ¨¡å‹çš„å¯¹è§†è§‰çš„ç†è§£ï¼Œèƒ½æå‡ä¸–ç•Œæ¨¡å‹çš„åœºæ™¯é¢„æµ‹ä¿çœŸåº¦ï¼›åœ¨LIBEROçš„æˆåŠŸç‡è¾¾97.4% | 
|  2025 |  æ¸…åå¤§å­¦  | [X-VLA](https://arxiv.org/pdf/2510.10274?)  |  åŒç³»ç»Ÿ  |  é€šè¿‡å¯å­¦ä¹ çš„Soft-Promptï¼ˆç¼–ç ç¡¬ä»¶ç‰¹æ€§ï¼‰ åµŒå…¥è®©æ¨¡å‹â€œç†è§£â€ä¸åŒæœºå™¨äººå’Œç¯å¢ƒçš„å·®å¼‚ï¼›0.9Bå‚æ•°é‡(LIBEROä¸Šå¹³å‡SRä¸º98.1%)ï¼›å®ç°120åˆ†é’Ÿæ— è¾…åŠ©è‡ªä¸»å è¡£æœ |
|  2025 | National University of Singapore | [VLA-4D](https://arxiv.org/pdf/2511.17199)  |  4D(VGGT+æ—¶é—´ç»´åº¦)+VLM(Qwen2.5-VL-7B,vision encoder + LLM)  | 4Dæ„ŸçŸ¥è§†è§‰è¡¨å¾ï¼Œè§†è§‰ç‰¹å¾ï¼ˆä¸‰ç»´ä½ç½®ä¿¡æ¯ï¼‰+ä¸€ç»´æ—¶é—´ä¿¡æ¯åµŒå…¥ï¼›æ—¶ç©ºåŠ¨ä½œè¡¨å¾ï¼ˆç©ºé—´åŠ¨ä½œè¡¨å¾æ‹“å±•äº†æ—¶åºä¿¡æ¯ç»´åº¦ï¼‰ |
|  2025 |  University of Maryland  | [TraceGen](https://arxiv.org/pdf/2511.21690)  |  DINOï¼ˆå‡ ä½•ç‰¹å¾ï¼‰+SigLIPï¼ˆè¯­ä¹‰ç‰¹å¾ï¼‰+T5ï¼ˆç¼–ç æŒ‡ä»¤ï¼‰+flow-based model  | å¼•å…¥â€œ3Dè½¨è¿¹ç©ºé—´â€ï¼ˆworld model/VGGTï¼‰ï¼Œå°†è§†é¢‘è½¬æ¢ä¸º3Dè½¨è¿¹ï¼›æŠŠé¢„æµ‹çš„3Dè½¨è¿¹ï¼Œé€šè¿‡é€†è¿åŠ¨å­¦è½¬æ¢æˆæœºå™¨äººçš„å…³èŠ‚è¿åŠ¨æŒ‡ä»¤ï¼Œç›´æ¥é©±åŠ¨æœºå™¨äººæ‰§è¡Œã€‚ä½†çœŸå®æœºå™¨äººä¸Šçš„æˆåŠŸç‡ä»ç„¶åªæœ‰67.5% |
| 2025 |  ä¸Šæµ·äº¤å¤§  | [Evo-1](https://arxiv.org/pdf/2511.04555)   |  VLM+cross-modulated diffusion transformer  | VLMä¸ºInternvl3(InternViT-300M+Qwen2.5-0.5B);é‡‡ç”¨ä¸¤é˜¶æ®µè®­ç»ƒï¼ˆfreeze VLMè®­ç»ƒaction expert+ full-scale fine-tuningï¼‰ï¼›ä»…0.77Bå‚æ•°åœ¨LIBEROä¸Šè·å–94.8%ï¼ŒMetaWorldï¼ˆ80.6%ï¼ŒPI0 47.9%ï¼‰å’ŒRoboTwinï¼ˆ37.8%ï¼ŒPI0 30.9%ï¼‰| 
|  2025 |  HKUST-GZ  | [Spatial Forcing](https://arxiv.org/pdf/2510.12276)  |  VGGT+PI0ï¼ˆVLM+action expertï¼‰ | éšå¼åœ°ç»™VLAèµ‹äºˆ3Dæ„ŸçŸ¥èƒ½åŠ›ï¼›åœ¨ä¸­é—´å±‚ï¼ˆç›¸å¯¹è¾ƒæ·±ä½†éæœ€æ·±ï¼‰é€šè¿‡æœ€å¤§åŒ–ä½™å¼¦ç›¸ä¼¼åº¦ï¼Œå¼ºåˆ¶è§†è§‰tokenä¸3Dç‰¹å¾çš„å¯¹é½ï¼›LIBERO benchmark4ä¸ªä»»åŠ¡çš„å¹³å‡æˆåŠŸç‡>98% |
|  2025 |  ä¸Šæµ·äº¤å¤§  | [Evo-0](https://arxiv.org/pdf/2507.00416?)  |  VGGT+PI0ï¼ˆVLM+action expertï¼‰  | å°†VGGTçš„å‡ ä½•ç‰¹å¾ä¸VLMè§†è§‰ç‰¹å¾é€šè¿‡èåˆæ¨¡å—ï¼ˆlightweight fusion modelï¼‰è¿›è¡Œèåˆï¼›åœ¨çœŸæœºæ‰å–å®éªŒå¸¦æ¥çš„æå‡28.53%->57.41% |
|  2025 |  åŒ—äº¬å¤§å­¦  | [EvoVLA](https://arxiv.org/pdf/2511.16166)  |  OpenVLA-OFTï¼ˆViT+LLMï¼‰+è‡ªç›‘ç£RL  | ä¸‰ä¸ªç»„ä»¶ï¼šRLï¼ˆé˜¶æ®µå¯¹é½å¥–åŠ±ã€åŸºäºä½å§¿çš„ç‰©ä½“æ¢ç´¢ï¼‰+é•¿æ—¶ç¨‹è®°å¿†æ¨¡å—ï¼ˆå¯æŸ¥è¯¢çš„æ•°æ®åº“ï¼‰ï¼›è§£å†³äº†VLAæ¨¡å‹çš„stage hallucinationï¼ˆå³å‡è£…å®Œæˆäº†æŸä¸ªä»»åŠ¡é˜¶æ®µè€Œè·å–å¥–åŠ±ï¼‰ï¼›å°†é•¿ç¨‹ä»»åŠ¡ï¼ˆæ­æ¡¥ã€å †å ã€æ£å­å…¥æ¯ï¼‰çš„å¹³å‡å‡†ç¡®ç‡æå‡åˆ°69.2% |
|  2025 |  Physical Intelligence   | [PI0.6](https://www.pi.website/download/pistar06.pdf)  |  PI0.5ï¼ˆVLM+action expertï¼‰+RL  | VLAé€šè¿‡å¼ºåŒ–å­¦ä¹ åœ¨ç°å®éƒ¨ç½²ä¸­å®ç°è‡ªæˆ‘æ”¹è¿›;åŸºäºç»éªŒä¸æ ¡æ­£çš„ä¼˜åŠ¿æ¡ä»¶ç­–ç•¥å¼ºåŒ–å­¦ä¹ (RECAP) ï¼›VLMæ¨¡å‹é‡‡ç”¨Gemma 3 4Bï¼ŒåŠ¨ä½œä¸“å®¶860Må‚æ•°é‡|
| 2025 |  ç¾å›¢  | [RoboTron-Mani](https://arxiv.org/pdf/2412.07215v1)  | 3D æ„ŸçŸ¥å¢å¼ºï¼ˆRoboDataæ•°æ®é›†+3Dæ„ŸçŸ¥æ¶æ„ï¼‰ + åŸºäºLLMçš„å¤šæ¨¡æ€èåˆæ¶æ„  | é€šè¿‡å¼•å…¥ç›¸æœºå‚æ•°çŸ«æ­£åŠoccupancyç›‘ç£æ¥å¢å¼º3Dç©ºé—´æ„ŸçŸ¥èƒ½åŠ› |
|  2025 |  Generalist  | [GEN-0](https://generalistai.com/blog/nov-04-2025-GEN-0)  |   Harmonic Reasoningæ¨¡å‹è¢«è®­ç»ƒåŒæ—¶æ¨ç†ä¸action | 27ä¸‡å°æ—¶çœŸå®ç‰©ç†äº¤äº’æ•°æ®è®­ç»ƒï¼›ï¼ˆæœºå™¨äººé¢†åŸŸï¼‰é¦–æ¬¡å‘ç°7Bå‚æ•°é‡ä»¥å†…æ¨¡å‹ä¼šå‡ºç°å›ºåŒ–ï¼Œè€Œè¶…è¿‡è¿™ä¸ªå‚æ•°é‡ï¼Œå¯å±•ç¤ºè‰¯å¥½Scaling Laws |
|2025|Dexmal|[Realtime-VLA](https://arxiv.org/pdf/2510.26742)| pi0(VLM+diffusion action expert)|cuda graph+simplified graph+optimized kernelsï¼›æ‰å–è½ç¬”ä»»åŠ¡æˆåŠŸç‡100%ï¼›RTX 4090 GPUå®ç°30HZæ¨ç†åŠup to 480HZåŠ¨ä½œç”Ÿæˆ|---|
|  2025 |  University of British Columbia  | [NanoVLA](https://arxiv.org/pdf/2510.25122v1)  |  VLM+action expert | è§†è§‰-è¯­è¨€è§£è€¦ï¼ˆåæœŸèåˆ+ç‰¹å¾ç¼“å­˜ï¼‰+é•¿çŸ­åŠ¨ä½œåˆ†å—+è‡ªé€‚åº”é€‰æ‹©éª¨å¹²ç½‘ç»œï¼›é¦–æ¬¡å®ç°åœ¨è¾¹ç¼˜è®¾å¤‡(Jetson Orin Nano)ä¸Šé«˜æ•ˆè¿è¡ŒVLAï¼ˆ41.6FPSï¼‰ï¼›|
| 2025 |  Shanghai AI Lab  | [InternVLA-M1](https://arxiv.org/pdf/2510.13778) |  VLM planner+action expertåŒç³»ç»Ÿ  | VLMæ˜¯é‡‡ç”¨äº†ç©ºé—´æ•°æ®è¿›è¡Œè®­ç»ƒçš„ï¼Œaction expertè¾“å‡ºå¯æ‰§è¡Œçš„ç”µæœºæŒ‡ä»¤ |
|2025|Figure AI |[Helix](https://www.figure.ai/news/helix)| VLM+Transformerï¼›å¿«æ…¢åŒç³»ç»Ÿ  | é¦–ä¸ªèƒ½è®©ä¸¤å°æœºå™¨äººåŒæ—¶ååŒå·¥ä½œçš„VLA æ¨¡å‹ï¼›æ§åˆ¶äººå½¢ä¸ŠåŠèº«|
|2025|Russia|[AnywhereVLA](https://arxiv.org/pdf/2509.21006)|SmolVLA+ä¼ ç»ŸSLAMå¯¼èˆª(Fast-LIVO2)+frontier-basedæ¢ç´¢|æ¶ˆè´¹çº§ç¡¬ä»¶ä¸Šå®æ—¶è¿è¡ŒVLAï¼›ç§»åŠ¨æœºæ¢°è‡‚|
|  2025 |  AgiBot-World  | [GO-1](https://arxiv.org/pdf/2503.06669?)  |  VLM+Action Expert  | AgiBot Worldï¼š5ä¸ªåœºæ™¯ä¸‹217ä¸ªtaskå¯¹åº”çš„ä¸€ç™¾ä¸‡æ¡çœŸå®æœºå™¨äººè½¨è¿¹ï¼›é€šè¿‡ latent action representationsæ¥æå‡æœºå™¨äººæ•°æ®çš„åˆ©ç”¨ï¼› |
|  2025 |  Physical Intelligence  | [PI0.5](https://openreview.net/pdf?id=vlhoswksBO)  |  PI0+PI-FAST+Hi Robot+å¤šæºå¼‚æ„æ•°æ®  | å¤šæºå¼‚æ„æ•°æ®è”åˆè®­ç»ƒ+åºåˆ—å»ºæ¨¡ç»Ÿä¸€æ¨¡æ€+å±‚æ¬¡è§„åˆ’æ¨ç†ï¼›é¦–ä¸ªå®ç°é•¿æœŸåŠçµå·§æœºæ¢°è‡‚æ“ä½œ|
|  2025 |  NVIDIA  | [GR00T N1.5](https://research.nvidia.com/labs/gear/gr00t-n1_5/)  |  åŒç³»ç»Ÿï¼› NVIDIA Eagle2.5 VLM + Diffusion Transformer  | VLMåœ¨å¾®è°ƒå’Œé¢„è®­ç»ƒçš„æ—¶å€™éƒ½frozen |
|  2025 |  NVIDIA  | [GR00T N1](https://arxiv.org/pdf/2503.14734)  |  åŒç³»ç»Ÿï¼›VLM(NVIDIA Eagle-2 VLM)+flow-matchingè®­ç»ƒçš„Diffusion Transformer  |  heterogeneous training data |
|  2025 |  KAIST  | [LAPA](https://arxiv.org/pdf/2410.11758?)  |  VLM  | é¦–ä¸ªé€šè¿‡æ— ç›‘ç£å­¦ä¹ ï¼ˆæ²¡æœ‰çœŸå€¼æœºå™¨äººaction labelï¼‰æ¥è®­ç»ƒVLAæ¨¡å‹çš„æ–¹æ³• |
|  2025 |  ç¾çš„  | [DexVLA](https://arxiv.org/pdf/2502.05855)  |  VLM+diffusion  | 1Bå‚æ•°é‡çš„diffusion expert with multi-headæ¶æ„ï¼Œå®ç°ä¸åŒå®ä½“å½¢æ€çš„å­¦ä¹ ï¼›ä¸‰é˜¶æ®µçš„åˆ†ç¦»å¼è®­ç»ƒç­–ç•¥ï¼›|
|  2025 |  ç¾çš„  | [DiVLA](https://openreview.net/pdf?id=VdwdU81Uzy)  |  VLM+autoregressive+diffusion  | autoregressiveè¿›è¡Œæ¨ç†ï¼Œè€Œdiffusionè¿›è¡ŒåŠ¨ä½œç”Ÿæˆä»¥æ§åˆ¶æœºå™¨äºº |
|  2025 |  ä¸Šæµ·AI Labä¸åŒ—äº¬äººå½¢  | [TinyVLA](https://arxiv.org/pdf/2409.12514)  |  ViT+LLM  | åœ¨OpenVLAåŸºç¡€ä¸Šå¼•å…¥è½»é‡VLMæ¨¡å‹ä»¥åŠdiffusion policy decoder | 
|  2025 |  Stanford  | [OpenVLA-OFT/OpenVLA-OFT+](https://arxiv.org/pdf/2502.19645)  |  ViT+LLM  | åœ¨OpenVLAåŸºç¡€ä¸Šå¼•å…¥äº†å¹¶è¡Œè§£ç ã€action chunkingã€è¿ç»­çš„åŠ¨ä½œè¡¨ç¤ºã€ç®€å•çš„L1å›å½’ä½œä¸ºè®­ç»ƒç›®æ ‡ï¼›å…¶ä¸­OpenVLA-OFT+åˆ™æ˜¯åœ¨SigLIPå’ŒDINOv2ä¹‹é—´æ’å…¥äº†FiLM |
|  2025 |  Physical Intelligence  | [Hi Robot](https://arxiv.org/pdf/2502.19417)  |  PI0+å¿«æ…¢åŒç³»ç»Ÿï¼ˆVLM+VLAï¼‰  | åˆ†å±‚äº¤äº’å¼æœºå™¨äººå­¦ä¹ ç³»ï¼Œå¯ä»¥æ‰§è¡Œé«˜å±‚æ¨ç†ä¸åº•å±‚ä»»åŠ¡æ‰§è¡Œ |
|  2025 |  Physical Intelligence  | [PI0-Fast/Ï€â‚€-FAST](https://arxiv.org/pdf/2501.09747)  |  PI0+é¢‘ç‡ç©ºé—´action Tokenization | æ¢ç´¢VLAè®­ç»ƒçš„action representationï¼›é€šè¿‡é¢‘åŸŸå¯¹åŠ¨ä½œåºåˆ—çš„TokenåŒ–ï¼Œå°†è®­ç»ƒæ—¶é—´å‡å°‘5å€ |
|  2024 |  Physical Intelligence  | [Ï€0/PI0](https://arxiv.org/pdf/2410.24164?)  |  VLM+action expertï¼ˆdiffusionï¼‰  | é€šæ‰æ¨¡å‹ï¼ˆgeneralist modelï¼‰ï¼›é¢„è®­ç»ƒ+task-specificå¾®è°ƒç­–ç•¥ |
|  2024 |  Stanford  | [OpenVLA](https://arxiv.org/pdf/2406.09246?)  |  SigLIPä¸DNIO-v2ä½œä¸ºè§†è§‰ç¼–ç å™¨ï¼Œå¤§è¯­è¨€æ¨¡å‹ï¼ˆLLaMA2-7Bï¼‰ä½œä¸ºé«˜å±‚æ¨ç†| é¦–ä¸ªå…¨é¢å¼€æºçš„é€šç”¨ VLA æ¨¡å‹ï¼Œç»“åˆå¤šæ¨¡æ€ç¼–ç ä¸å¤§è¯­è¨€æ¨¡å‹æ¶æ„ï¼›é¦–æ¬¡å±•ç¤ºäº†é€šè¿‡ä½ç§©é€‚åº”ï¼ˆLoRAï¼‰å’Œæ¨¡å‹é‡åŒ–ç­‰è®¡ç®—é«˜æ•ˆçš„å¾®è°ƒæ–¹æ³•ï¼Œå®ç°é™ä½è®¡ç®—æˆæœ¬ä¸”ä¸å½±å“æˆåŠŸç‡ |
|  2024 |  UC Berkeley  | [Octo](https://arxiv.org/pdf/2405.12213)  |  Transformer  | é‡‡ç”¨diffusionä½œä¸ºè¿ç»­åŠ¨ä½œç”Ÿæˆï¼›åŸºäºOpen x-embodimentè®­ç»ƒçš„å¤§å‹æ¶æ„ï¼›é€šç”¨æœºå™¨äººæ¨¡å‹çš„æ¢ç´¢|
|  2024 |  å­—èŠ‚  | [GR-2](https://arxiv.org/pdf/2410.06158?)  | GPT-style è§†é¢‘ç”Ÿæˆæ¨¡å‹   | GR-1çš„æ™‹çº§ç‰ˆï¼Œç”¨äº†æ›´å¤šã€æ›´diversityçš„æ•°æ®æ¥é¢„è®­ç»ƒ |
|  2024 |  å­—èŠ‚  | [GR-1](https://arxiv.org/pdf/2312.13139)  |  GPT-style è§†é¢‘ç”Ÿæˆæ¨¡å‹  | é€šè¿‡GPTå¼ç”Ÿæˆæ¨¡å‹çš„é¢„è®­ç»ƒï¼ˆvideo generative pre-training modelï¼‰ï¼Œç»“åˆæœºå™¨äººæ•°æ®è¿›è¡Œå¾®è°ƒ |
|  2024 |  Stanford  | [ReKep](https://arxiv.org/pdf/2409.01652)  |  ViT+VLM  | DINOv2æå–3Då…³é”®ç‚¹ï¼ŒVLMé€šè¿‡æŒ‡ä»¤ä¸å›¾åƒç”Ÿæˆå…³é”®ç‚¹ä¸ç©ºé—´çš„çº¦æŸï¼Œé€šè¿‡æ±‚è§£ä¼˜åŒ–è·å–æœºå™¨äººæœ«ç«¯æ‰§è¡Œè½¨è¿¹ |
|  2023 |  Google DeepMind  | [RT-2](https://robotics-transformer2.github.io/assets/rt2.pdf)  |  VLM  | æ­£å¼æå‡ºVLAæ¦‚å¿µï¼›é‡‡ç”¨VLMä½œä¸ºéª¨æ¶ï¼›Internet-scaleé¢„è®­ç»ƒVLMæ¨¡å‹åœ¨æœºå™¨äººæ§åˆ¶ä¸Šå±•ç¤ºè‰¯å¥½çš„æ³›åŒ–æ€§åŠè¯­ä¹‰æ¨ç†ï¼›å°†actionä¹Ÿè¡¨è¾¾æˆæ–‡æœ¬tokençš„å½¢å¼ |
|2023|Stanford|[ALOHA/ACT](https://arxiv.org/pdf/2304.13705)|CVAE+Transformer|åŠ¨ä½œåˆ†å—ï¼›ç”¨ä½æˆæœ¬å¹³å°å®ç°ç²¾ç»†æ“ä½œ,å¦‚çº¿æ‰å¸¦ã€ä¹’ä¹“çƒ|
|  2023 |  Stanford  | [VoxPoser](https://arxiv.org/pdf/2307.05973)  |  LLM+VLM  | LLMè¿›è¡Œä»£ç ç”Ÿæˆé©±åŠ¨æœºå™¨äººå®Œæ•´æ“ä½œä»»åŠ¡ï¼ŒVLMè·å–3Dä»·å€¼åœ°å›¾è·å–å®ç°ä»»åŠ¡çš„è½¨è¿¹è§„åˆ’ |
|  2023 |  Google  | [SayCan](https://proceedings.mlr.press/v205/ichter23a/ichter23a.pdf)  |  LLM  | æ¢ç´¢å¦‚ä½•åˆ©ç”¨ LLM å®ç°å¯¹æœºå™¨äººåŠ¨ä½œçš„æ§åˆ¶,é€šè¿‡é¢„å®šä¹‰çš„è¿åŠ¨ï¼ˆmotion primitivesï¼‰æ¥ä¸ç¯å¢ƒè¿›è¡Œäº¤äº’ |
|2023|Google DeepMind|[RT-1](https://arxiv.org/pdf/2212.06817)|EfficientNet+Transformer|VLAä»»åŠ¡é¦–æ¬¡ç”¨åˆ°å®é™…æœºæ¢°è‡‚|


~~~
PS: VLAçš„æ–¹æ³•å®åœ¨å¤ªå¤šäº†ï¼Œåç»­çœ‹åˆ°æœ‰æ„æ€çš„å·¥ä½œä¼šåŠæ—¶æ›´æ–°æ­¤è¡¨æ ¼ï¼Œä½†æ–¹æ³•è§£è¯»å®åœ¨æ²¡åŠæ³•éƒ½æ•´ç†ï¼Œåªèƒ½å›«å›µåæ£ï¼Œé”™æ¼ä¹‹å¤„ï¼Œæ¬¢è¿è¯„è®ºåŒºæŒ‡å‡ºğŸ™
~~~

## VLAå¸¸ç”¨çš„æ•°æ®é›†

<!-- |---|`arXiv`|---|---|---| -->
<!-- [![Github stars](https://img.shields.io/github/stars/***.svg)]() -->

| Year | Venue | Paper Title | Repository | Note |
|:----:|:-----:| ----------- |:----------:|:----:|
|2025|`arXiv`|[RoboWheel: A Data Engine from Real-World Human Demonstrations for Cross-Embodiment Robotic Learning](https://arxiv.org/pdf/2512.02729)|[![Github stars](https://img.shields.io/github/stars/zhangyuhong01/Robowheel-Toolkits.svg)](https://github.com/zhangyuhong01/Robowheel-Toolkits)<br>[website](https://zhangyuhong01.github.io/Robowheel)<br>[dataset](https://huggingface.co/datasets/HORA-DB/HORA)|æ•°æ®å¼•æ“ï¼Œå°†æ™®é€šå•ç›® RGB/RGB-D ç›¸æœºæ‹æ‘„çš„äººç±»æ‰‹-ç‰©äº¤äº’ï¼ˆHOIï¼‰è§†é¢‘ï¼Œè½¬åŒ–ä¸ºé€‚ç”¨äºå·¥ä¸šæœºæ¢°è‡‚ã€çµå·§æ‰‹ã€äººå½¢æœºå™¨äººç­‰ä¸åŒå½¢æ€è®¾å¤‡çš„è®­ç»ƒæ•°æ®ï¼Œæ— éœ€å¤æ‚ç¡¬ä»¶å³å¯å®ç°åª²ç¾é¥æ“ä½œçš„è®­ç»ƒæ•ˆæœã€‚|
|2025|`arXiv`|[Galaxea open-world dataset and g0 dual-system vla model](https://arxiv.org/pdf/2509.00576)|[![Github stars](https://img.shields.io/github/stars/OpenGalaxea/GalaxeaVLA.svg)](https://github.com/OpenGalaxea/GalaxeaVLA)|[website](https://opengalaxea.github.io/GalaxeaVLA/)<br>500+å°æ—¶çœŸå®æœºå™¨äººæ•°æ®|
|2025|`RSS`|[Robomind: Benchmark on multi-embodiment intelligence normative data for robot manipulation](https://arxiv.org/pdf/2412.13877)|[![Github stars](https://img.shields.io/github/stars/x-humanoid-robomind/x-humanoid-robomind.github.io.svg)](https://github.com/x-humanoid-robomind/x-humanoid-robomind.github.io)|[website](https://x-humanoid-robomind.github.io/)|
|2025|`arXiv`|[InternData-A1: Pioneering High-Fidelity Synthetic Data for Pre-training Generalist Policy](https://arxiv.org/pdf/2511.16651)|[huggingface](https://huggingface.co/datasets/InternRobotics/InternData-A1)|[website](https://internrobotics.github.io/interndata-a1.github.io/)|
|2025|`arXiv`|[Robotwin 2.0: A scalable data generator and benchmark with strong domain randomization for robust bimanual robotic manipulation](https://arxiv.org/pdf/2506.18088)|---|[website](https://robotwin-platform.github.io/)|
|2025|`IROS`|[Agibot world colosseo: A large-scale manipulation platform for scalable and intelligent embodied systems](https://arxiv.org/pdf/2503.06669)|[![Github stars](https://img.shields.io/github/stars/OpenDriveLab/AgiBot-World.svg)](https://github.com/OpenDriveLab/AgiBot-World)|[website](https://agibot-world.com/) <br> AgiBot World dataset|
|2025|`RSS`|[Robomind: Benchmark on multi-embodiment intelligence normative data for robot manipulation](https://arxiv.org/pdf/2412.13877)|[![Github stars](https://img.shields.io/github/stars/x-humanoid-robomind/x-humanoid-robomind.github.io.svg)](https://github.com/x-humanoid-robomind/x-humanoid-robomind.github.io)|[website](https://x-humanoid-robomind.github.io/)|
|2025|`ICRA`|[Dexmimicgen: Automated data generation for bimanual dexterous manipulation via imitation learning](https://arxiv.org/pdf/2410.24185)|[![Github stars](https://img.shields.io/github/stars/NVlabs/dexmimicgen.svg)](https://github.com/NVlabs/dexmimicgen/)|[website](https://dexmimicgen.github.io/)<br>DexMimicGen|
|2024|`ICRA`|[Rh20t: A comprehensive robotic dataset for learning diverse skills in one-shot](https://rh20t.github.io/static/RH20T_paper_compressed.pdf)|[![Github stars](https://img.shields.io/github/stars/rh20t/rh20t_api.svg)](https://github.com/rh20t/rh20t_api)|[website](https://rh20t.github.io/)| 
|2024|`RSS`|[Robocasa: Large-scale simulation of everyday tasks for generalist robots](https://arxiv.org/pdf/2406.02523)|[![Github stars](https://img.shields.io/github/stars/robocasa/robocasa.svg)](https://github.com/robocasa/robocasa)|[website](https://robocasa.ai/)|
|2024|`RSS`|[Droid: A large-scale in-the-wild robot manipulation dataset](https://arxiv.org/pdf/2403.12945)|---|[website](https://droid-dataset.github.io/)|
|2024|`ICRA`|[Roboagent: Generalization and efficiency in robot manipulation via semantic augmentations and action chunking](https://arxiv.org/pdf/2309.01918)|[![Github stars](https://img.shields.io/github/stars/robopen/roboagent.svg)](https://github.com/robopen/roboagent/)|[website](https://robopen.github.io/)|
|2023|`NIPS`|[Libero: Benchmarking knowledge transfer for lifelong robot learning](https://proceedings.neurips.cc/paper_files/paper/2023/file/8c3c666820ea055a77726d66fc7d447f-Paper-Datasets_and_Benchmarks.pdf)|---|[website](https://libero-project.github.io/)<br>LIBERO|
|2023|`CoRL`|[Bridgedata v2: A dataset for robot learning at scale](https://proceedings.mlr.press/v229/walke23a/walke23a.pdf)|[![Github stars](https://img.shields.io/github/stars/rail-berkeley/bridge_data_v2.svg)](https://github.com/rail-berkeley/bridge_data_v2)|[website](https://rail-berkeley.github.io/bridgedata/)<br>WidowX|
|2023|`CoRL`|[Open x-embodiment: Robotic learning datasets and rt-x models](https://arxiv.org/pdf/2310.08864)|[![Github stars](https://img.shields.io/github/stars/google-deepmind/open_x_embodiment.svg)](https://github.com/google-deepmind/open_x_embodiment)|[website](https://robotics-transformer-x.github.io/)|
|2023|`CoRL`|[Rt-2: Vision-language-action models transfer web knowledge to robotic control](https://robotics-transformer2.github.io/assets/rt2.pdf)|---|[Website](https://robotics-transformer2.github.io/)|
|2022|`RAL`|[Calvin: A benchmark for language-conditioned policy learning for long-horizon robot manipulation tasks](https://arxiv.org/pdf/2112.03227)|[![Github stars](https://img.shields.io/github/stars/mees/calvin.svg)](https://github.com/mees/calvin)|[website](http://calvin.cs.uni-freiburg.de/)|
|2022|`arXiv`|[Rt-1: Robotics transformer for real-world control at scale](https://arxiv.org/pdf/2212.06817)|[![Github stars](https://img.shields.io/github/stars/google-research/robotics_transformer.svg)](https://github.com/google-research/robotics_transformer)|[website](https://robotics-transformer1.github.io/) <br> Google robot|
|2022|`CoRL`|[BC-Z: Zero-shot task generalization with robotic imitation learning](https://proceedings.mlr.press/v164/jang22a/jang22a.pdf)|---|[website](https://sites.google.com/view/bc-z/home)|
|2022|`RSS`|[Bridge data: Boosting generalization of robotic skills with cross-domain datasets](https://arxiv.org/pdf/2109.13396)|[![Github stars](https://img.shields.io/github/stars/yanlai00/bridge_data_imitation_learning.svg)](https://github.com/yanlai00/bridge_data_imitation_learning) <br> [![Github stars](https://img.shields.io/github/stars/yanlai00/bridge_data_robot_infra.svg)](https://github.com/yanlai00/bridge_data_robot_infra) |[website](https://sites.google.com/view/bridgedata) <br> Google robot|
|2025|`CoRL`|[Meta-world: A benchmark and evaluation for multi-task and meta reinforcement learning](https://proceedings.mlr.press/v100/yu20a/yu20a.pdf)|[![Github stars](https://img.shields.io/github/stars/Farama-Foundation/Metaworld.svg)](https://github.com/Farama-Foundation/Metaworld)|[website](https://metaworld.farama.org/)|
|2019|`CoRL`|[Robonet: Large-scale multi-robot learning](https://arxiv.org/pdf/1910.11215)|---|[website](https://www.robonet.wiki/)|


<!-- |---|`arXiv`|---|---|---| -->
<!-- [![Github stars](https://img.shields.io/github/stars/***.svg)]() -->

